[laiser]
; required if you are running this as a server with the web GUI
BINDING_ADDRESS = 192.168.0.196
BINDING_PORT = 1313

; You must use one of llama.cpp or ollama.
; You can fill out one or both, doesn't matter.

[llamaCPP]
LLAMA_IP = 192.168.0.196
LLAMA_PORT = 8080

[ollama]
; URL without any endpoints:
OLLAMA_BASE_URL = http://localhost:11434
; change to your URL:
OLLAMA_URL = http://localhost:11434/api/generate
; the model you're using:
OLLAMA_MODEL = stablelm-zephyr:latest

[default_API]
; 'llama.cpp' or 'ollama', depending which you're using
API_TO_USE = llama.cpp

[status_messages]
; output the answer or error only. 'true' to only print the response
silent = false

[advanced]
; Changing these settings could negatively affect accuracy.
; This is considered to be "tuned" already,
; so playing with them is experimental.

; Amount of search results to fetch:
SEARCH_RESULT_COUNT = 3
; Amount of news results to fetch:
NEWS_RESULT_COUNT = 3
; trim summary to N amount of lines:
TRIM_WIKIPEDIA_SUMMARY = True
; number of lines to trim to:
TRIM_WIKIPEDIA_LINES = 7
